{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"This is simmilar to forward selection, except when a new variable is\n",
    "inserted, we check the p-values for the previously inserted regressors to ensure they are still significant\"\"\"\n",
    "\n",
    "def stepwise_section_v1(Y, X, alpha):\n",
    "    \n",
    "    # a list to store the included regressors\n",
    "    # the constant is already included\n",
    "    included_regressors = [0]\n",
    "    \n",
    "     #keeps track of the remaining regressors\n",
    "    remaining_regressors = list(range(1, np.shape(X)[1]))\n",
    "    \n",
    "    #initialize the \"best\" model with just the constant\n",
    "    final_results = sm.OLS(Y, X[:, included_regressors]).fit()\n",
    "    \n",
    "    \n",
    "    while len(remaining_regressors) > 0:\n",
    "    \n",
    "        #increase the number of included regressors, and initialize with \n",
    "        #the first remaining regressor\n",
    "        included_regressors.append(remaining_regressors[0])\n",
    "\n",
    "        #initialize the temp best model for this level in the forward selection process\n",
    "        temp_best_model = sm.OLS(Y, X[:, included_regressors]).fit()\n",
    "\n",
    "        #initialize the best regressor for this level\n",
    "        best_regressor = remaining_regressors[0]\n",
    "        #consider the remaining regressors, and find the best one to choose next \n",
    "        for i in range(1, len(remaining_regressors)):\n",
    "            #update the included_regressors to use the current regressor\n",
    "            included_regressors[-1] = remaining_regressors[i]\n",
    "            #create the results for the model\n",
    "            temp_model = sm.OLS(Y, X[:, included_regressors]).fit()\n",
    "            #see if the current regressor is better than the current best by reducing ssr\n",
    "            if temp_model.ssr < temp_best_model.ssr:\n",
    "                temp_best_model = temp_model\n",
    "                best_regressor = included_regressors[-1]\n",
    "                \n",
    "        #check if the regressor should be added to the model, if so we will continue to add regressors\n",
    "        if temp_best_model.pvalues[-1] < alpha:\n",
    "            final_results = temp_best_model\n",
    "            remaining_regressors.remove(best_regressor)\n",
    "            included_regressors[-1] = best_regressor\n",
    "            \n",
    "            #now we need to add functionality that eliminates any included regressors that are\n",
    "            #not the best. We skip the constant\n",
    "            for i in range(1, len(included_regressors)):\n",
    "                #remove any previous regressors who are no longer significant\n",
    "                if final_results.pvalues[i] > alpha:\n",
    "                    included_regressors.pop(i)\n",
    "            final_results = sm.OLS(Y, X[:, included_regressors])\n",
    "        #if the regressor does not meet the threshold, then terminate the loop and return\n",
    "        else:\n",
    "            del included_regressors[-1]\n",
    "            return included_regressors, final_results\n",
    "    \n",
    "    return included_regressors, final_results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_excel('NFL_Data.xlsx', sheet_name = 'Data')\n",
    "df_n = df.to_numpy()\n",
    "X = df_n[:, 1:(df_n.shape[1])]\n",
    "Y = df_n[:, [0]]\n",
    "#add a column of 1's to the front to account for the constant\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors, my_results= stepwise_section_v1(Y, X, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 8, 2, 7]\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.786\n",
      "Model:                            OLS   Adj. R-squared:                  0.760\n",
      "Method:                 Least Squares   F-statistic:                     29.44\n",
      "Date:                Fri, 07 Jan 2022   Prob (F-statistic):           3.27e-08\n",
      "Time:                        18:49:34   Log-Likelihood:                -52.532\n",
      "No. Observations:                  28   AIC:                             113.1\n",
      "Df Residuals:                      24   BIC:                             118.4\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.8084      7.901     -0.229      0.821     -18.115      14.498\n",
      "x1            -0.0048      0.001     -3.771      0.001      -0.007      -0.002\n",
      "x2             0.0036      0.001      5.177      0.000       0.002       0.005\n",
      "x3             0.1940      0.088      2.198      0.038       0.012       0.376\n",
      "==============================================================================\n",
      "Omnibus:                        0.665   Durbin-Watson:                   1.492\n",
      "Prob(Omnibus):                  0.717   Jarque-Bera (JB):                0.578\n",
      "Skew:                           0.321   Prob(JB):                        0.749\n",
      "Kurtosis:                       2.712   Cond. No.                     7.42e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.42e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(regressors)\n",
    "print(my_results.fit().summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
