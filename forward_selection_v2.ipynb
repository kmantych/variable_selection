{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\" forward_selection_v2 differs from v1 in several ways.\n",
    "1. It is assumed that the function contains a constant. \n",
    "2. It selects the next regressor by comparing ssr values of the models with the\n",
    "same number of regressors. The one with the smallest ssr value is selected for the next step\n",
    "3. Once the next regressor is selected, if the p-value for the t-statistic is smaller than alpha, then the regressor is\n",
    "added to the model. \n",
    "4. The loop ends when a regressor is found that does not meet the condition outlined in (3). This means that the potential\n",
    "result that is returned only contains the constant\n",
    "5. It returns results instead of a model\"\"\"\n",
    "\n",
    "def forward_section_v2(Y, X, alpha):\n",
    "    \n",
    "    # a list to store the included regressors\n",
    "    # the constant is already included\n",
    "    included_regressors = [0]\n",
    "    \n",
    "     #keeps track of the remaining regressors\n",
    "    remaining_regressors = list(range(1, np.shape(X)[1]))\n",
    "    \n",
    "    #initialize the \"best\" model with just the constant\n",
    "    final_results = sm.OLS(Y, X[:, included_regressors]).fit()\n",
    "    \n",
    "    \n",
    "    while len(remaining_regressors) > 0:\n",
    "    \n",
    "        #increase the number of included regressors, and initialize with \n",
    "        #the first remaining regressor\n",
    "        included_regressors.append(remaining_regressors[0])\n",
    "\n",
    "        #initialize the temp best model for this level in the forward selection process\n",
    "        temp_best_model = sm.OLS(Y, X[:, included_regressors]).fit()\n",
    "\n",
    "        #initialize the best regressor for this level\n",
    "        best_regressor = remaining_regressors[0]\n",
    "        #consider the remaining regressors, and find the best one to choose next \n",
    "        for i in range(1, len(remaining_regressors)):\n",
    "            #update the included_regressors to use the current regressor\n",
    "            included_regressors[-1] = remaining_regressors[i]\n",
    "            #create the results for the model\n",
    "            temp_model = sm.OLS(Y, X[:, included_regressors]).fit()\n",
    "            #see if the current regressor is better than the current best by reducing ssr\n",
    "            if temp_model.ssr < temp_best_model.ssr:\n",
    "                temp_best_model = temp_model\n",
    "                best_regressor = included_regressors[-1]\n",
    "                \n",
    "        #check if the regressor should be added to the model, if so we will continue to add regressors\n",
    "        if temp_best_model.pvalues[-1] < alpha:\n",
    "            final_results = temp_best_model\n",
    "            remaining_regressors.remove(best_regressor)\n",
    "            included_regressors[-1] = best_regressor\n",
    "        #if the regressor does not meet the threshold, then terminate the loop and return\n",
    "        else:\n",
    "            del included_regressors[-1]\n",
    "            return included_regressors, final_results\n",
    "    \n",
    "    return included_regressors, final_results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_excel('Crime_R.xlsx')\n",
    "df_n = df.to_numpy()\n",
    "X = df_n[:, 1:(df_n.shape[1])]\n",
    "Y = df_n[:, [0]]\n",
    "#add a column of 1's to the front to account for the constant\n",
    "X = sm.add_constant(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020906925201416016\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "data = sm.datasets.longley.load()\n",
    "data.exog = sm.add_constant(data.exog)\n",
    "\n",
    "start = time.time()\n",
    "#regressors, my_results= forward_section_v2(data.endog, data.exog, 0.05)\n",
    "regressors, my_results= forward_section_v2(Y, X, 0.05)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.994\n",
      "Model:                            OLS   Adj. R-squared:                  0.994\n",
      "Method:                 Least Squares   F-statistic:                     3700.\n",
      "Date:                Mon, 08 Nov 2021   Prob (F-statistic):           9.45e-50\n",
      "Time:                        17:09:12   Log-Likelihood:                -103.69\n",
      "No. Observations:                  47   AIC:                             213.4\n",
      "Df Residuals:                      44   BIC:                             218.9\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         15.2343      5.052      3.016      0.004       5.053      25.415\n",
      "x1             0.7307      0.009     85.189      0.000       0.713       0.748\n",
      "x2             0.0230      0.009      2.567      0.014       0.005       0.041\n",
      "==============================================================================\n",
      "Omnibus:                        0.942   Durbin-Watson:                   0.513\n",
      "Prob(Omnibus):                  0.624   Jarque-Bera (JB):                0.985\n",
      "Skew:                           0.230   Prob(JB):                        0.611\n",
      "Kurtosis:                       2.460   Cond. No.                     8.78e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.78e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(my_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
